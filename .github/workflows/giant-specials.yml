name: Giant Specials to Google Sheets

on:
  schedule:
    # Example: Tue & Fri 09:30 America/New_York â‰ˆ 13:30 UTC during EDT; adjust as needed
    - cron: "30 13 * * 2,5"
  workflow_dispatch: {}

jobs:
  run-scraper:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    env:
      GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/service_account.json

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Show repo listing (sanity check)
        run: ls -la

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright browsers (with OS deps)
        run: |
          python -m playwright install --with-deps chromium

      # Write service account key from GitHub secret
      - name: Write service account key (raw JSON)
        env:
          SA_JSON: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_JSON }}
        run: |
          set -euo pipefail
          # Write the JSON exactly as provided
          printf '%s' "$SA_JSON" > "$GOOGLE_APPLICATION_CREDENTIALS"
          # Validate JSON so the job fails early if malformed/empty
          python -c "import json,os; p=os.environ['GOOGLE_APPLICATION_CREDENTIALS']; json.load(open(p)); print('[INFO] service_account.json looks valid. Size:', len(open(p,'rb').read()), 'bytes')"
          
      - name: Run scraper (unbuffered to flush logs)
        run: |
          python -u giant_specials_to_sheets.py

      # Collect artifacts (CSV/XLSX when it works; screenshot/HTML if not)
      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-run
          path: |
            *.csv
            *.xlsx
            page.png
            page.html
          if-no-files-found: ignore
